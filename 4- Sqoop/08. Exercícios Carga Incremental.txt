#Realizar com uso do MySQL

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it database bash 
root@database:/# mysql -psecret 
mysql> use sakila;

##---------------------------------------------------------------
#1. Criar a tabela cp_rental_append, contendo a cópia da tabela rental com os campos rental_id e rental_date
##---------------------------------------------------------------

mysql> create table cp_rental_append as select rental_id, rental_date from rental;
Query OK, 16044 rows affected (0.54 sec)
Records: 16044  Duplicates: 0  Warnings: 0

##---------------------------------------------------------------
#2 .Criar a tabela cp_rental_id e cp_rental_date, contendo a cópia da tabela cp_rental_append
##---------------------------------------------------------------
 
mysql> create table cp_rental_id as select * from cp_rental_append;
Query OK, 16044 rows affected (0.38 sec)
Records: 16044  Duplicates: 0  Warnings: 0

mysql> create table cp_rental_date as select * from cp_rental_append;
Query OK, 16044 rows affected (0.37 sec)
Records: 16044  Duplicates: 0  Warnings: 0


#Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it namenode bash 


##---------------------------------------------------------------
#3. Importar as tabelas cp_rental_append, cp_rental_id e cp_rental_date com 1 mapeador
##---------------------------------------------------------------
 
sqoop import --table cp_rental_append \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3


sqoop import --table cp_rental_id \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3


sqoop import --table cp_rental_date \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3


root@namenode:/# hdfs dfs -ls -h -R /user/hive/warehouse/db_test_t3
drwxr-xr-x   - root supergroup          0 2021-09-07 12:58 /user/hive/warehouse/db_test_t3/cp_rental_append
-rw-r--r--   3 root supergroup          0 2021-09-07 12:58 /user/hive/warehouse/db_test_t3/cp_rental_append/_SUCCESS
-rw-r--r--   3 root supergroup    427.9 K 2021-09-07 12:58 /user/hive/warehouse/db_test_t3/cp_rental_append/part-m-00000
drwxr-xr-x   - root supergroup          0 2021-09-07 13:01 /user/hive/warehouse/db_test_t3/cp_rental_date
-rw-r--r--   3 root supergroup          0 2021-09-07 13:01 /user/hive/warehouse/db_test_t3/cp_rental_date/_SUCCESS
-rw-r--r--   3 root supergroup    427.9 K 2021-09-07 13:01 /user/hive/warehouse/db_test_t3/cp_rental_date/part-m-00000
drwxr-xr-x   - root supergroup          0 2021-09-07 13:00 /user/hive/warehouse/db_test_t3/cp_rental_id
-rw-r--r--   3 root supergroup          0 2021-09-07 13:00 /user/hive/warehouse/db_test_t3/cp_rental_id/_SUCCESS
-rw-r--r--   3 root supergroup    427.9 K 2021-09-07 13:00 /user/hive/warehouse/db_test_t3/cp_rental_id/part-m-00000


#Realizar com uso do MySQL

##---------------------------------------------------------------
#4. Executar o sql /db-sql/sakila/insert_rental.sql no container do database
##---------------------------------------------------------------

$ docker exec -it database bash
$ cd /db-sql/sakila
$ mysql -psecret < insert_rental.sql


#Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it namenode bash


##---------------------------------------------------------------
#5. Atualizar a tabela cp_rental_append no HDFS anexando os novos arquivos
##---------------------------------------------------------------

sqoop import --table cp_rental_append \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3 \
--append


##---------------------------------------------------------------
#6. Atualizar a tabela cp_rental_id no HDFS de acordo com o último registro de rental_id, adicionando apenas os novos dados.
##---------------------------------------------------------------

sqoop eval \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--query "select * from cp_rental_append order by rental_id desc limit 2"


sqoop import --table cp_rental_id \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3 \
--incremental append \
--check-column rental_id \
--last-value 16049

##---------------------------------------------------------------
#7. Atualizar a tabela cp_rental_date no HDFS de acordo com o último registro de rental_date, atualizando os registros a partir desta data.
##---------------------------------------------------------------

sqoop import --table cp_rental_date \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--m 1 \
--warehouse-dir /user/hive/warehouse/db_test_t3 \
--incremental lastmodified \
--merge-key rental_id \
--check-column rental_date \
--last-value '2005-08-23 22:50:12'
