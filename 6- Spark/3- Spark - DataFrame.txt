##---------------------------------------------------------------
#1. Enviar o diretório local “/input/exercises-data/juros_selic” para o HDFS em “/user/aluno/<nome>/data”
##---------------------------------------------------------------

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it namenode bash
root@namenode:/# hdfs dfs -put /input/exercises-data/juros_selic/ /user/aluno/aline/data


##---------------------------------------------------------------
#2. Criar o DataFrame jurosDF para ler o arquivo no HDFS “/user/aluno/<nome>/data/juros_selic/juros_selic.json”
##---------------------------------------------------------------

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it spark bash
root@spark:/# spark-shell

scala> val jurosDF = spark.read.json("/user/aluno/aline/data/juros_selic/juros_selic.json")
jurosDF: org.apache.spark.sql.DataFrame = [data: string, valor: string] 


##---------------------------------------------------------------
#3. Visualizar o Schema do jurosDF
##---------------------------------------------------------------

scala> jurosDF.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)


##---------------------------------------------------------------
#4. Mostrar os 5 primeiros registros do jutosDF
##---------------------------------------------------------------

scala> jurosDF.show(7,false)
+----------+-----+
|data      |valor|
+----------+-----+
|01/06/1986|1.27 |
|01/07/1986|1.95 |
|01/08/1986|2.57 |
|01/09/1986|2.94 |
|01/10/1986|1.96 |
|01/11/1986|2.37 |
|01/12/1986|5.47 |
+----------+-----+
only showing top 7 rows


##---------------------------------------------------------------
#5. Contar a quantidade de registros do jurosDF
##---------------------------------------------------------------

scala> jurosDF.count()
res3: Long = 393

##---------------------------------------------------------------
#6. Criar o DataFrame jurosDF10 para filtrar apenas os registros com o campo “valor” maior que 10
##---------------------------------------------------------------

scala> val jurosDF10 = jurosDF.filter("valor > 10")
jurosDF10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [data: string, valor: string]


scala> jurosDF10.show(4)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
+----------+-----+
only showing top 4 rows


##---------------------------------------------------------------
#7. Salvar o DataFrame jurosDF10  como tabela Hive “<nome>.tab_juros_selic”
##---------------------------------------------------------------

jurosDF10.write.saveAsTable("db_aline.tab_juros_selic")

##---------------------------------------------------------------
#8. Criar o DataFrame jurosHiveDF para ler a tabela “<nome>.tab_juros_selic”
##---------------------------------------------------------------

scala> var jurosHiveDF = spark.read.table("db_aline.tab_juros_selic")
jurosHiveDF: org.apache.spark.sql.DataFrame = [data: string, valor: string]


##---------------------------------------------------------------
#9. Visualizar o Schema do jurosHiveDF
##---------------------------------------------------------------

scala> jurosHiveDF.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)


##---------------------------------------------------------------
#10. Mostrar os 5 primeiros registros do jurosHiveDF
##---------------------------------------------------------------

scala> jurosHiveDF.show(10)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
|01/05/1987|24.63|
|01/06/1987|18.02|
|01/11/1987|12.92|
|01/12/1987|14.38|
|01/01/1988|16.78|
|01/02/1988|18.35|
+----------+-----+
only showing top 10 rows


##---------------------------------------------------------------
#11. Salvar o DataFrame jurosHiveDF no HDFS no diretório “/user/aluno/nome/data/save_juros” no formato parquet
##---------------------------------------------------------------

jurosHiveDF.write.parquet("/user/aluno/aline/data/save_juros")

##---------------------------------------------------------------
#12. Visualizar o save_juros no HDFS
##---------------------------------------------------------------

aline@aline-virtualbox:~/treinamentos/docker-bigdata$ docker exec -it namenode bash
root@namenode:/# hdfs dfs -ls /user/aluno/aline/data/save_juros
Found 2 items
-rw-r--r--   2 root supergroup          0 2021-09-09 12:28 /user/aluno/aline/data/save_juros/_SUCCESS
-rw-r--r--   2 root supergroup       1419 2021-09-09 12:28 /user/aluno/aline/data/save_juros/part-00000-33db55c4-4e06-4599-a685-da7c83eed36f-c000.snappy.parquet


##---------------------------------------------------------------
#13. Criar o DataFrame jurosHDFS para ler o diretório do “save_juros” da questão 8
##---------------------------------------------------------------

scala> val jurosHDFS = spark.read.parquet("/user/aluno/aline/data/save_juros")
jurosHDFS: org.apache.spark.sql.DataFrame = [data: string, valor: string]


##---------------------------------------------------------------
#14. Visualizar o Schema do jurosHDFS
##---------------------------------------------------------------

scala> jurosHDFS.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)


##---------------------------------------------------------------
#15. Mostrar os 5 primeiros registros do jurosHDFS
##---------------------------------------------------------------

scala> jurosHDFS.show(5)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
|01/05/1987|24.63|
+----------+-----+
only showing top 5 rows
